{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FD2NWHamnNzM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ahmad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,f1_score,recall_score,precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import os\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Psgh0Tr8nQ7w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='D:\\Data Science\\Interview Assignment/dataset.csv'\n",
    "df=pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JmXAsj4hnmd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ziXU5bPvgKBd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "language    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M4RRfFI_gOmK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pushto        1000\n",
       "French        1000\n",
       "Persian       1000\n",
       "Latin         1000\n",
       "Russian       1000\n",
       "Estonian      1000\n",
       "Japanese      1000\n",
       "Hindi         1000\n",
       "Swedish       1000\n",
       "Indonesian    1000\n",
       "Romanian      1000\n",
       "English       1000\n",
       "Spanish       1000\n",
       "Thai          1000\n",
       "Chinese       1000\n",
       "Turkish       1000\n",
       "Urdu          1000\n",
       "Tamil         1000\n",
       "Portugese     1000\n",
       "Arabic        1000\n",
       "Korean        1000\n",
       "Dutch         1000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NvvOh_f-p4v6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arabic', 'Chinese', 'Dutch', 'English', 'Estonian', 'French',\n",
       "       'Hindi', 'Indonesian', 'Japanese', 'Korean', 'Latin', 'Persian',\n",
       "       'Portugese', 'Pushto', 'Romanian', 'Russian', 'Spanish', 'Swedish',\n",
       "       'Tamil', 'Thai', 'Turkish', 'Urdu'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(df['language'], return_counts=True)\n",
    "unique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create feature : length of language and length of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zw51bPNwnxmB"
   },
   "outputs": [],
   "source": [
    "df['length']=df['Text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4KT0RrrTn5KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def punctuation_count(txt):\n",
    "  count=sum(1 for c in txt if c in string.punctuation)\n",
    "  return count\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YYrEZGbwn6oQ"
   },
   "outputs": [],
   "source": [
    "df['length_punc']=df['Text'].apply((lambda x: punctuation_count(x) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8gPrUswdn9Ar"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "      <th>length</th>\n",
       "      <th>length_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "      <td>305</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  language  length  \\\n",
       "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian     339   \n",
       "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish     171   \n",
       "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai     251   \n",
       "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil     305   \n",
       "4  de spons behoort tot het geslacht haliclona en...     Dutch     176   \n",
       "\n",
       "   length_punc  \n",
       "0            0  \n",
       "1            3  \n",
       "2            0  \n",
       "3            3  \n",
       "4            0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning the text from puncutation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RE_PUNCT = re.compile('([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
    "def strip_punctuation(s):\n",
    "    return RE_PUNCT.sub(\" \", s)\n",
    "\n",
    "RE_NUMERIC = re.compile(r\"[0-9]+\", re.UNICODE)\n",
    "def strip_numeric(s):\n",
    "    return RE_NUMERIC.sub(\"\", s)\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaning_functions = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "    for f in cleaning_functions:\n",
    "        text = f(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['Text']\n",
    "language = df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [clean_text(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting data into train 70% and test 30 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mpDoKYZZoR3h"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentences, \n",
    "                                                    language,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization using (Tf-idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3UcfqWVWorpS"
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer( analyzer='char', ngram_range=(1,3), lowercase=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASMtd3yhOjCn"
   },
   "source": [
    " # **building language detection model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBz91bKXNeCL"
   },
   "source": [
    "# Randomforest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sBGjRz2f04zy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  15.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 2 of 2) Processing clf, total= 7.0min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [20,30,40,50],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "rfc=RandomForestClassifier()\n",
    "model_rfc= GridSearchCV(rfc, param_grid=param_grid)\n",
    "text_rfc= Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model_rfc)\n",
    "                    ],verbose=True)\n",
    "text_rfc.fit(X_train,y_train)\n",
    "\n",
    "#predictions on test data\n",
    "predictions_rfc=text_rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8V4wIhW7SmMI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(model_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-mtg5EQvM8h9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9687878787878788"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predictions_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1P2hdeQPM8iE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.99      1.00      1.00       300\n",
      "     Chinese       0.97      0.92      0.95       291\n",
      "       Dutch       0.96      0.97      0.97       313\n",
      "     English       0.72      0.99      0.83       289\n",
      "    Estonian       0.98      0.94      0.96       308\n",
      "      French       0.96      0.98      0.97       287\n",
      "       Hindi       1.00      0.98      0.99       314\n",
      "  Indonesian       0.99      0.95      0.97       295\n",
      "    Japanese       0.95      0.99      0.97       302\n",
      "      Korean       1.00      0.98      0.99       296\n",
      "       Latin       0.98      0.86      0.92       312\n",
      "     Persian       0.99      0.99      0.99       299\n",
      "   Portugese       0.96      0.93      0.95       293\n",
      "      Pushto       1.00      0.96      0.98       303\n",
      "    Romanian       0.99      0.97      0.98       291\n",
      "     Russian       0.98      1.00      0.99       302\n",
      "     Spanish       0.98      0.98      0.98       287\n",
      "     Swedish       0.99      1.00      0.99       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.98      0.99       305\n",
      "     Turkish       1.00      0.99      0.99       298\n",
      "        Urdu       1.00      0.98      0.99       324\n",
      "\n",
      "    accuracy                           0.97      6600\n",
      "   macro avg       0.97      0.97      0.97      6600\n",
      "weighted avg       0.97      0.97      0.97      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_rfc, target_names=unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tOZwSMBVM8iI"
   },
   "outputs": [],
   "source": [
    "f1_score_rfc=f1_score(y_test,predictions_rfc,average='weighted')\n",
    "precision_score_rfc=precision_score(y_test,predictions_rfc,average='weighted')\n",
    "recall_score_rfc=recall_score(y_test,predictions_rfc,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V542feqVODfi"
   },
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kf7GLNQGOBb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  14.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=11.1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern=...\n",
       "                                                               min_impurity_decrease=0.0,\n",
       "                                                               min_impurity_split=None,\n",
       "                                                               min_samples_leaf=1,\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                               presort=False,\n",
       "                                                               random_state=None,\n",
       "                                                               splitter='best'),\n",
       "                              iid='warn', n_jobs=None,\n",
       "                              param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                                           'max_depth': [1, 2, 3, 4, 5, 6, 7]}],\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt=DecisionTreeClassifier()\n",
    "\n",
    "param_grid =[{'criterion':['gini','entropy'],'max_depth':[1,2,3,4,5,6,7]}]\n",
    "model_dt = GridSearchCV(dt, param_grid=param_grid)\n",
    "text_dt = Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model_dt),\n",
    "                    ],verbose=True)\n",
    "text_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ei3D0hBpVHdx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "print(model_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Tdllk9mRONql"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259090909090909"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dt=text_dt.predict(X_test)\n",
    "accuracy_score(y_test,predictions_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "h2MvbzAHONqr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       0.99      1.00      1.00       300\n",
      "     Chinese       0.99      0.96      0.98       291\n",
      "       Dutch       0.74      0.87      0.80       313\n",
      "     English       0.71      0.90      0.79       289\n",
      "    Estonian       0.80      0.88      0.84       308\n",
      "      French       0.93      0.89      0.91       287\n",
      "       Hindi       0.95      0.98      0.96       314\n",
      "  Indonesian       0.96      0.89      0.92       295\n",
      "    Japanese       1.00      0.98      0.99       302\n",
      "      Korean       1.00      0.96      0.98       296\n",
      "       Latin       0.66      0.82      0.73       312\n",
      "     Persian       0.99      0.98      0.98       299\n",
      "   Portugese       0.97      0.83      0.89       293\n",
      "      Pushto       0.99      0.94      0.96       303\n",
      "    Romanian       1.00      0.92      0.96       291\n",
      "     Russian       0.98      0.99      0.99       302\n",
      "     Spanish       0.93      0.91      0.92       287\n",
      "     Swedish       0.98      0.81      0.89       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.97      0.98       305\n",
      "     Turkish       1.00      0.94      0.97       298\n",
      "        Urdu       1.00      0.96      0.98       324\n",
      "\n",
      "    accuracy                           0.93      6600\n",
      "   macro avg       0.94      0.93      0.93      6600\n",
      "weighted avg       0.94      0.93      0.93      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_dt, target_names=unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yOWZRfBkONqu"
   },
   "outputs": [],
   "source": [
    "f1_score_dt=f1_score(y_test,predictions_rfc,average='weighted')\n",
    "precision_score_dt=precision_score(y_test,predictions_dt,average='weighted')\n",
    "recall_score_dt=recall_score(y_test,predictions_dt,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K-SEWaQ0Mtt"
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wbfBRpcKovcM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  14.0s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "text_clf = Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model)\n",
    "                    ],verbose=True)\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SXxeaA30oz_y"
   },
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_sCBdWOGo2qG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768181818181818"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CP54yqvvpTfU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      1.00      1.00       300\n",
      "     Chinese       0.99      0.98      0.99       291\n",
      "       Dutch       0.98      0.97      0.98       313\n",
      "     English       0.72      1.00      0.84       289\n",
      "    Estonian       1.00      0.95      0.97       308\n",
      "      French       0.96      0.99      0.97       287\n",
      "       Hindi       1.00      0.98      0.99       314\n",
      "  Indonesian       1.00      0.96      0.98       295\n",
      "    Japanese       1.00      0.99      0.99       302\n",
      "      Korean       1.00      0.99      0.99       296\n",
      "       Latin       0.99      0.90      0.94       312\n",
      "     Persian       0.99      1.00      0.99       299\n",
      "   Portugese       0.99      0.94      0.96       293\n",
      "      Pushto       1.00      0.96      0.98       303\n",
      "    Romanian       1.00      0.98      0.99       291\n",
      "     Russian       0.99      1.00      0.99       302\n",
      "     Spanish       0.98      0.99      0.98       287\n",
      "     Swedish       1.00      1.00      1.00       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.98      0.99       305\n",
      "     Turkish       1.00      0.99      0.99       298\n",
      "        Urdu       1.00      0.98      0.99       324\n",
      "\n",
      "    accuracy                           0.98      6600\n",
      "   macro avg       0.98      0.98      0.98      6600\n",
      "weighted avg       0.98      0.98      0.98      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions, target_names=unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "t9L5fd7k0MuO"
   },
   "outputs": [],
   "source": [
    "f1_score_nb=f1_score(y_test,predictions,average='weighted')\n",
    "precision_score_nb=precision_score(y_test,predictions,average='weighted')\n",
    "recall_score_nb=recall_score(y_test,predictions,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTLw1TvI0Mud"
   },
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "E1ad6hi2p_dA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 2 of 2) Processing clf, total= 2.3min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#define the model and parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {'n_neighbors':[3,4,5,6,7]}\n",
    "\n",
    "#Fit the model\n",
    "model_knn = GridSearchCV(knn, param_grid=parameters)\n",
    "text_knn = Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model_knn),\n",
    "                    ],verbose=True)\n",
    "text_knn.fit(X_train,y_train)\n",
    "\n",
    "#predictions on test data\n",
    "predictions_knn=text_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qvonFjBAXTwr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "print(model_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "MWjHHKNoubF7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768181818181818"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predictions_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_c6udrFL0Mus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      1.00      1.00       300\n",
      "     Chinese       0.99      0.97      0.98       291\n",
      "       Dutch       0.97      0.98      0.98       313\n",
      "     English       0.77      0.98      0.86       289\n",
      "    Estonian       0.99      0.94      0.96       308\n",
      "      French       0.96      0.98      0.97       287\n",
      "       Hindi       1.00      0.98      0.99       314\n",
      "  Indonesian       0.98      0.99      0.98       295\n",
      "    Japanese       0.99      0.98      0.98       302\n",
      "      Korean       1.00      0.99      0.99       296\n",
      "       Latin       0.97      0.91      0.94       312\n",
      "     Persian       0.99      1.00      0.99       299\n",
      "   Portugese       0.98      0.96      0.97       293\n",
      "      Pushto       0.99      0.97      0.98       303\n",
      "    Romanian       1.00      0.97      0.99       291\n",
      "     Russian       0.98      1.00      0.99       302\n",
      "     Spanish       0.97      0.99      0.98       287\n",
      "     Swedish       1.00      1.00      1.00       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.98      0.99       305\n",
      "     Turkish       1.00      0.98      0.99       298\n",
      "        Urdu       1.00      0.98      0.99       324\n",
      "\n",
      "    accuracy                           0.98      6600\n",
      "   macro avg       0.98      0.98      0.98      6600\n",
      "weighted avg       0.98      0.98      0.98      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_knn, target_names=unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1rr-H5UK0Mux"
   },
   "outputs": [],
   "source": [
    "f1_score_knn=f1_score(y_test,predictions_knn,average='weighted')\n",
    "precision_score_knn=precision_score(y_test,predictions_knn,average='weighted')\n",
    "recall_score_knn=recall_score(y_test,predictions_knn,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKrbLces0MvX"
   },
   "source": [
    "# logestic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "IscuITxc0MvY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  14.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=36.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern=...\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='multinomial',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False),\n",
       "                              iid='warn', n_jobs=None,\n",
       "                              param_grid={'C': [0.01, 0.1, 1, 10]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid={'C': [0.01, 0.1, 1, 10] }\n",
    "logis_model=LogisticRegression(multi_class='multinomial',solver='lbfgs')\n",
    "model_logis=GridSearchCV(logis_model, param_grid=param_grid)\n",
    "text_logis = Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model_logis),\n",
    "                    ],verbose=True)\n",
    "text_logis.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hvxYdwk7Xc2V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n"
     ]
    }
   ],
   "source": [
    "print(model_logis.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "EJDlWAZV0Mvc"
   },
   "outputs": [],
   "source": [
    "predictions_logis=text_logis.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aIdpHhGd0Mvk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834848484848485"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predictions_logis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dwx9mUw30Mvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      1.00      1.00       300\n",
      "     Chinese       0.99      0.99      0.99       291\n",
      "       Dutch       0.98      0.98      0.98       313\n",
      "     English       0.86      0.98      0.91       289\n",
      "    Estonian       0.99      0.96      0.98       308\n",
      "      French       0.96      0.99      0.97       287\n",
      "       Hindi       1.00      0.98      0.99       314\n",
      "  Indonesian       0.99      0.99      0.99       295\n",
      "    Japanese       1.00      0.99      1.00       302\n",
      "      Korean       1.00      0.99      0.99       296\n",
      "       Latin       0.95      0.95      0.95       312\n",
      "     Persian       1.00      1.00      1.00       299\n",
      "   Portugese       0.98      0.98      0.98       293\n",
      "      Pushto       1.00      0.97      0.98       303\n",
      "    Romanian       1.00      0.98      0.99       291\n",
      "     Russian       0.98      1.00      0.99       302\n",
      "     Spanish       0.98      0.98      0.98       287\n",
      "     Swedish       1.00      1.00      1.00       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.98      0.99       305\n",
      "     Turkish       1.00      0.99      0.99       298\n",
      "        Urdu       1.00      0.98      0.99       324\n",
      "\n",
      "    accuracy                           0.98      6600\n",
      "   macro avg       0.98      0.98      0.98      6600\n",
      "weighted avg       0.98      0.98      0.98      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_logis, target_names=unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "99hu_QYX0Mvs"
   },
   "outputs": [],
   "source": [
    "f1_score_logis=f1_score(y_test,predictions_logis,average='weighted')\n",
    "precision_score_logis=precision_score(y_test,predictions_logis,average='weighted')\n",
    "recall_score_logis=recall_score(y_test,predictions_logis,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## support vector machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing tfidf, total=  14.7s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=27.5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 3), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=SVC(C=10.0, cache_size=200,\n",
       "                                                   class_weight='balanced',\n",
       "                                                   coef0=0.0,\n",
       "                                                   decision_function_shape='ovr',\n",
       "                                                   degree=3, gamma=0.01,\n",
       "                                                   kernel='linear', max_iter=-1,\n",
       "                                                   probability=True,\n",
       "                                                   random_state=None,\n",
       "                                                   shrinking=True, tol=0.001,\n",
       "                                                   verbose=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model_svm = OneVsRestClassifier(svm.SVC(gamma=0.01, C=10., probability=True, class_weight='balanced', kernel='linear'))\n",
    "text_svm = Pipeline([('tfidf', tfidf_vect),\n",
    "                    ('clf', model_svm),\n",
    "                    ],verbose=True)\n",
    "text_svm.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm=text_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860606060606061"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,predictions_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Arabic       1.00      1.00      1.00       300\n",
      "     Chinese       0.99      0.99      0.99       291\n",
      "       Dutch       0.99      0.98      0.99       313\n",
      "     English       0.89      0.99      0.93       289\n",
      "    Estonian       0.99      0.97      0.98       308\n",
      "      French       0.97      0.98      0.98       287\n",
      "       Hindi       1.00      0.99      0.99       314\n",
      "  Indonesian       0.99      0.99      0.99       295\n",
      "    Japanese       1.00      0.99      1.00       302\n",
      "      Korean       1.00      0.99      1.00       296\n",
      "       Latin       0.96      0.96      0.96       312\n",
      "     Persian       1.00      1.00      1.00       299\n",
      "   Portugese       0.97      0.99      0.98       293\n",
      "      Pushto       1.00      0.97      0.99       303\n",
      "    Romanian       1.00      0.98      0.99       291\n",
      "     Russian       0.98      1.00      0.99       302\n",
      "     Spanish       0.98      0.98      0.98       287\n",
      "     Swedish       1.00      1.00      1.00       290\n",
      "       Tamil       1.00      0.99      0.99       301\n",
      "        Thai       1.00      0.98      0.99       305\n",
      "     Turkish       0.99      0.99      0.99       298\n",
      "        Urdu       1.00      0.99      0.99       324\n",
      "\n",
      "    accuracy                           0.99      6600\n",
      "   macro avg       0.99      0.99      0.99      6600\n",
      "weighted avg       0.99      0.99      0.99      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_svm, target_names=unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "99hu_QYX0Mvs"
   },
   "outputs": [],
   "source": [
    "f1_score_svm=f1_score(y_test,predictions_svm,average='weighted')\n",
    "precision_score_svm=precision_score(y_test,predictions_svm,average='weighted')\n",
    "recall_score_svm=recall_score(y_test,predictions_svm,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show  Tthe  Algorithm’s Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jFgr0rVj0Mvz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Algorithm</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.973414</td>\n",
       "      <td>0.968788</td>\n",
       "      <td>0.969774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decicion Tree</th>\n",
       "      <td>0.935032</td>\n",
       "      <td>0.925909</td>\n",
       "      <td>0.969774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.981436</td>\n",
       "      <td>0.976818</td>\n",
       "      <td>0.977937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.979585</td>\n",
       "      <td>0.976818</td>\n",
       "      <td>0.977473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic</th>\n",
       "      <td>0.984473</td>\n",
       "      <td>0.983485</td>\n",
       "      <td>0.983733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Svm</th>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.986061</td>\n",
       "      <td>0.986195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Algorithm      precision    recall  f1_score\n",
       "Random Forest   0.973414  0.968788  0.969774\n",
       "Decicion Tree   0.935032  0.925909  0.969774\n",
       "Naive_Bayes     0.981436  0.976818  0.977937\n",
       "KNN             0.979585  0.976818  0.977473\n",
       "logistic        0.984473  0.983485  0.983733\n",
       "Svm             0.986627  0.986061  0.986195"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_precision=[precision_score_rfc,precision_score_dt,precision_score_nb,precision_score_knn,precision_score_logis,precision_score_svm]\n",
    "list_recall=[recall_score_rfc,recall_score_dt,recall_score_nb,recall_score_knn,recall_score_logis,recall_score_svm]\n",
    "list_f1=[f1_score_rfc,f1_score_dt,f1_score_nb,f1_score_knn,f1_score_logis,f1_score_svm]\n",
    "\n",
    "\n",
    "df_1=pd.DataFrame(list_precision, index=['Random Forest','Decicion Tree','Naive_Bayes','KNN','logistic','Svm'])\n",
    "df_1.columns =['precision']\n",
    "df_1.insert(loc=1,column='recall',value=list_recall)\n",
    "df_1.insert(loc=2,column='f1_score',value=list_f1)\n",
    "df_1.columns.name = 'Algorithm'\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a predictions with Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Dell is an American multinational computer technology company'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['戴爾是美國跨國計算機技術公司'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Portugese'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Dell é uma empresa multinacional americana de tecnologia da computação'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hindi'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['डेल एक अमेरिकी बहुराष्ट्रीय कंप्यूटर प्रौद्योगिकी कंपनी है'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Dell - американская транснациональная компьютерная компания.'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Korean'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Dell은 미국의 다국적 컴퓨터 기술 회사입니다.'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Swedish'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Dell är ett amerikanskt multinationellt datateknikföretag'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
